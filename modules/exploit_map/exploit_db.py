"""
Search Exploit-DB for available exploits
"""

import asyncio
from typing import Dict, Any, List
from loguru import logger

try:
    import aiohttp
except ImportError:
    aiohttp = None


class ExploitDBSearch:
    """Search Exploit-DB for exploits"""
    
    def __init__(self, config):
        self.config = config
        self.base_url = 'https://www.exploit-db.com'
        self.api_url = 'https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv'
        
    async def search(self, cve_id: str) -> List[Dict[str, Any]]:
        """Search for exploits by CVE ID"""
        if not aiohttp:
            logger.error("aiohttp not installed")
            return []
        
        logger.info(f"Searching Exploit-DB for {cve_id}")
        
        try:
            exploits = []
            
            # Search via web scraping
            exploits.extend(await self._search_web(cve_id))
            
            # Search by service/software name if available
            if hasattr(self, 'service_name'):
                exploits.extend(await self._search_by_service(self.service_name))
            
            logger.success(f"Found {len(exploits)} exploits for {cve_id}")
            return exploits
            
        except Exception as e:
            logger.error(f"Exploit-DB search failed: {e}")
            return []
    
    async def _search_web(self, cve_id: str) -> List[Dict[str, Any]]:
        """Search Exploit-DB website"""
        exploits = []
        
        # Clean CVE ID
        cve_clean = cve_id.replace('CVE-', '').replace('cve-', '')
        
        search_url = f'{self.base_url}/search?cve={cve_clean}'
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(search_url, timeout=30) as resp:
                    if resp.status == 200:
                        html = await resp.text()
                        
                        # Parse HTML for exploit entries (simplified)
                        # In production, use BeautifulSoup for proper parsing
                        if 'exploit' in html.lower():
                            # Extract exploit IDs from HTML
                            import re
                            edb_ids = re.findall(r'/exploits/(\d+)', html)
                            
                            for edb_id in set(edb_ids[:10]):  # Limit to 10
                                exploit_info = await self._get_exploit_details(edb_id)
                                if exploit_info:
                                    exploits.append(exploit_info)
        
        except Exception as e:
            logger.debug(f"Web search failed: {e}")
        
        return exploits
    
    async def _get_exploit_details(self, edb_id: str) -> Dict[str, Any]:
        """Get exploit details"""
        try:
            async with aiohttp.ClientSession() as session:
                url = f'{self.base_url}/exploits/{edb_id}'
                async with session.get(url, timeout=30) as resp:
                    if resp.status == 200:
                        html = await resp.text()
                        
                        # Extract title (simplified)
                        import re
                        title_match = re.search(r'<h1[^>]*>([^<]+)</h1>', html)
                        title = title_match.group(1) if title_match else f'Exploit {edb_id}'
                        
                        return {
                            'id': f'EDB-{edb_id}',
                            'name': title.strip(),
                            'url': url,
                            'download_url': f'{self.base_url}/download/{edb_id}',
                            'type': 'exploit',
                            'platform': self._extract_platform(html),
                            'verified': 'verified' in html.lower()
                        }
        except Exception as e:
            logger.debug(f"Failed to get exploit details: {e}")
        
        return None
    
    def _extract_platform(self, html: str) -> str:
        """Extract platform from HTML"""
        platforms = ['linux', 'windows', 'macos', 'unix', 'multiple', 'hardware', 'web']
        
        html_lower = html.lower()
        for platform in platforms:
            if platform in html_lower:
                return platform
        
        return 'unknown'
    
    async def _search_by_service(self, service_name: str) -> List[Dict[str, Any]]:
        """Search by service/software name"""
        exploits = []
        
        try:
            search_url = f'{self.base_url}/search?q={service_name}'
            
            async with aiohttp.ClientSession() as session:
                async with session.get(search_url, timeout=30) as resp:
                    if resp.status == 200:
                        html = await resp.text()
                        
                        import re
                        edb_ids = re.findall(r'/exploits/(\d+)', html)
                        
                        for edb_id in set(edb_ids[:5]):  # Limit to 5
                            exploit_info = await self._get_exploit_details(edb_id)
                            if exploit_info:
                                exploits.append(exploit_info)
        
        except Exception as e:
            logger.debug(f"Service search failed: {e}")
        
        return exploits
    
    async def search_by_software(self, software: str, version: str = None) -> List[Dict[str, Any]]:
        """Search for exploits by software name and version"""
        query = software
        if version:
            query = f'{software} {version}'
        
        logger.info(f"Searching Exploit-DB for {query}")
        
        self.service_name = query
        return await self._search_by_service(query)
